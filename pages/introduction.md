---
layout: default
title: Introduction
permalink: introduction
---
<!-- Add an essay or interpretive material below this line,
using HTML or markdown.  Do not modify this file above this line -->
## Introduction

<p>Annotation is one of John Unsworth’s scholarly primitives: processes essential for research, pedagogy, and interpretation. Although primarily considered in relation to printed texts, it’s also useful for scholars working with digital sound recordings, wherein various forms of annotation can serve various functions, from metadata that provides contextualizing information, to transcriptions which remediate sound, to personal, timestamped notes. These forms of annotation are specific to different contexts, such as the format of the audio recording, the affordances of the repository in which it is displayed, and the listening paradigm of the creator. Because annotation is not an ahistorical, incidental consequence of interacting with audio online but rather a situated artifact of a given listening experience, it holds within it also the possibility of critical reflexivity about an audiotext and its various forms of mediation. Typical theories of annotation tend not to consider the affordances of annotation when it is not an artifact of “authorial intention,” (as in paratext and occasionally marginalia) or else a means of making an item “system-aware” (as in metadata schemes used in online repositories) (Clement and Fischer, par. 1, par. 3). However, recent scholarship has produced both theories (e.g. Clement and Fischer’s “audiated annotation”) and case studies (e.g. The SpokenWeb Digital Anthology, published on AVAnnotate) illustrating the critical significance of annotation as a practice in itself. Based on these developments, I use annotation in this project as a means to think reflexively about how I represent the specific features of a given audiotext and the specific forms of mediation that inform my listening. I find that a consciously reflexive annotation process leads to annotations that reflect their own situatedness.</p>

<p>	This AVAnnotate project is a case study in reflexive annotation. Here, I present annotations of three recordings of Oana Avasilichioaei’s performance at the 2019 SpokenWeb Symposium. My annotation is grounded in audiotextual features, including polyphony, semiotic features, and sonic stratification; and markers of my own mediated listening practice, which was focused on relative differences through use of the digital editing application Audacity. Although each recording is of the same performance, they come from recording devices placed differently in the room. By reflexively attending to the audiotextual and mediated specificities of each audiotext, I produce annotations that highlight the distinct features of each, rather than reducing them to a common or essential set of sounds. This case study is an indication that reflexive annotation can assert audiotextual multiplicity, and highlight the multimedial and (frequently) unauthorized elements that make it possible to listen to and annotate audio. I begin by discussing how features of the audiotexts, such as polyvocality, ambient sound, vocal modulation, fluctuations, semantic features, and novel sounds, inform my selection of a common annotation genre and language, but varying topics and layers. Next, I address how the technological specificities of my listening experience inform my use of relative language and changing precision in my timestamps. Each topic I discuss includes examples from my “Operator” annotations. </p>

## Audiotextual Features
<p>While there are several potential forms audio annotation can take, the selection of a given form and vocabulary to describe sound isn't typically discussed as a result of interpretive and/or critical strategies. Rather, it’s a result of functional needs (e.g. transcripts for a podcast) or technological affordances (e.g. the parameters of a given metadata scheme). However, tools like AVAnnotate, which support a variety of forms of annotation, allow users creative latitude to define their own format (within certain limits) and vocabulary. Different ways of annotating sound are informed by different critical histories and interpretive goals. For instance, describing sound through the lens of prosody situates annotations in a specific form of poetic criticism which assumes consistency and exactitude (Bernstein 14). Another form, transcription of semantic language, in “demand[ing] a certain degree of envoicement from the reader,” may obscure the semiotic qualities of the audiotext (Camlot and Mitchell). Furthermore, elements like timestamps may represent different temporalities depending on whether they’re strictly chronological or allow for gaps and overlaps in time. The critical significance of forms and vocabularies of annotation, then, is not only in their affordances as means of representation, but also in how they constitute sound. In the following discussion, I describe some elements of the “Operator” audiotexts and explain how I used them to inform the structure of my annotations.</p>

<p><i>
Layered timestamps and indexes
</i></p>
<p>At any one moment in any of the audiotexts, there are multiple layers of sound. These range from an electronic bass tone to voices to static distortion. They become audible and inaudible at different times, change at varying intervals—in these ways, they exist in time independently from each other. For this reason, I’ve timestamped them independently. This creates a non-chronological temporality in my timestamps, in that a sound cannot be said to have completely occurred before the next sound, only to have begun before the next. Consider, for instance, this section from Mic 1:</p>
<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:01:40.63</td>
    <td>0:02:92.26</td> 
    <td>Sporadic mid-volume rolling sound, like a pool ball rolling across a table</td>
  </tr>
  <tr>
    <td>0:01:54.0</td>
    <td>0:01:54.4</td> 
    <td>Quiet breathy/blowing sound</td>
  </tr>
  <tr>
    <td>0:01:59.193</td>
    <td>0:01:59.59</td> 
    <td>Loud tap, like two pool balls colliding</td>
  </tr>
</table>
<p>One moment in time, such as 0:01:50, corresponds to multiple indices and thus has multiple layers, none of which—in the Mic 1 audiotext—are subordinate to each other even when they are infrequent/sporadic (as in the first entry above), quiet (as in the second) or short (as in the third). Mic 3 has similar interactions between layers of sounds, although they blend together a bit more, leading to more annotations that group sounds together.
Mic 2 is also polyphonic, but it has clearer audio quality and consequently clearly defined layers of sound. Because the layers of sound are more distinct, I am able to identify layers not just based on time (i.e. through simultaneous timestamps) but also by shared sonic characteristics between sounds at different points in the audiotext. These shared qualities inform my use of indexes to denote four general categories of sound: speaking voices, silence, electronic tones and distortions, and friction sounds. The latter two are represented in this sample:
</p>
<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:01:35.78</td>
    <td>0:02:03.32</td> 
    <td>Gravelley, crunching sound beginning with a period of ~0.5 second. Low to mid pitch and volume. Period becomes longer and volume decreases through this interval.</td>
    <td>Friction sounds</td>
  </tr>
  <tr>
    <td>0:01:40.66</td>
    <td>0:02:02.04</td> 
    <td>Clear scratch sound with varying pitch and generally low volume, like a wooden stick tracing across a wooden table.</td>
    <td>Friction sounds</td>
  </tr>
  <tr>
    <td>0:02:10.01</td>
    <td>0:02:11:71</td> 
    <td>Polyphonic electronic tone crescendos to high volume</td>
    <td>Electronic tones and distortions</td>
  </tr>
</table>
<p>By attending to the polyphonic nature of each of these audiotexts, and the distinct manner in which sounds stratify, blend, and remediate each other in these audiotexts, I reflect the variances in sonic layering in each.</p>

<p><i>Focus on semiotic features</i></p>
<p>	In each of the audiotexts, the majority of the sounds are not symbolic speech. And, when spoken language is audible, it’s complicated by semiotic features, whether from other layers of sound or striking voice modulation. For this reason, my annotation focuses on semiotic features of sound. This is most evident in my descriptions of speech, which use terms of volume, pitch, onset/offset, tone, and (occasionally) echo. Here I draw on Charles Bernstein’s description of a “poetic mode of listening,” in which there is “an oscillation (or temporal overlap) between the materially present sound…and the absent meaning” (18). It’s for this reason, also, that I rarely use spacial descriptors (i.e. background, foreground) and I don’t often attribute sounds to specific sources. This practice is informed, in part, by Michel Chion’s reduced listening, which centers around “inherent qualities of sounds,” (29) and which Chion places at odds with causal listening (i.e. listening for information about the source) and semantic listening (i.e. listening for linguistic meaning). 
	While Chion distinguishes between these as three separate forms of listening, my annotations are closer to, in Bernstein’s description, “oscillati[ng]” between them. This is true in the sense that none of my descriptors are directly indicative of innate features of sound, but instead the necessity of essentializing certain features of it, using constructed ways of describing sound (e.g. volume and pitch), in order to be able to write about it. It’s also true in the sense that there are causal and semantic descriptions mixed into my semiotic descriptions. In annotations for each audiotext, I use timbre descriptors like “electronic,” “brassy,” “ and “crunching,” which imply causes even if they don’t state them outright. When I use more specific images, I make them similes, as in “mid-pitch clicking sound, like ball bearings colliding” (Mic 2). 
Because Mic 1 has the clearest speech, it has more descriptors of symbolic features of speech, as in the following example:
</p>
<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:03:12.83</td>
    <td>0:03:43.5</td> 
    <td>One mid pitch voice speaking. Little intonation (i.e. most syllables are stressed), resulting in choppy-sounding words (e.g. ve-hi-cules). Shout pauses between words. Call-and-response form (e.g. "...right? Yeah, that's what it looks like to me.") suggests two speakers, but there's only one voice. Terms that presumably refer to descriptive rather than spoken text (e.g. "unintelligible," "explicative") included within flow of speech, uttered by same speaker.</td>
  </tr>
</table>
<p>Here, I discuss a symbolic “call and response form” which is contradicted by semiotic features denoting a single voice. Additionally, I isolate words which I assume to “refer to descriptive rather than spoken text,” implicitly referring to their disruption of the syntactic structures of the speech, and to the forms of audio transcription which typically use descriptors like [unintelligible]. In this case, including these descriptions highlights how the audiotext draws attention to the blurry line between the semiotic and the semantic: does “unintelligible” function the same way as other symbolic words in this context, or is it closer to an index? When questions like this arise from the audiotext, they are obscured by a sole focus on semiotic features of speech. I alter my descriptions, when needed, to fit these complexities of the audiotext. 
Bernstein’s discussion of “absent meaning” is particularly relevant to the Mic 3 audiotext, in which spoken words are frequently inaudible (in the sense that they can’t be made out as symbolic words). To seek the symbolic in these sounds really does feel like reaching for something that’s not present among a plethora of “materially present sound[s],” more than are in either of the other audiotexts. For this reason, I’ve foregone the descriptor “Oana’s voice” in my annotations of this audiotext, not because her voice is unrecognizable, but because this descriptor is causative (i.e. sound is attributed to voice, which is ultimately attributed to a causative speaker) and privileges the voice relative to other sounds, when it does not function in a privileged manner in this audiotext itself, but rather is mixed into the sonic din. Because Oana’s voice is separated by volume, pitch, and clarity from the other sounds in the Mic 1 and Mic 2 audiotexts, I use the “Oana’s voice” descriptor to indicate this. However, I maintain my focus on semiotic features of sound across all of my annotations in recognition of the skillful manner in which she manipulates her voice, imbuing it with semiotic meaning.
Of course, the semiotic elements of a voiced performance are not predicated on authorial intent. What I’m suggesting here is, instead, that this audiotext is in many ways about the relationship between the symbolic and the semiotic; at what point does whispered offset become simply a whisper in itself, or when (if ever) does a repeated word lose its meaning? The Operator audiotexts interrogate the constructed boundaries between the semiotic and the semantic in complex, intermingling ways, which I can only begin to approach in my written annotations.</p>

<p><i>General and relative language</i></p>
<p>The difficulty of describing literary sound has been noted by Bernstein, who writes that “our technical vocabulary strains at accounting for more than a small portion of the acoustic activity of the sounded poem” (15). The Operator audiotexts, on the other hand, are open to unauthorized sound—because of the polyvocal, variable performance style, there is no clear delimitation, particularly in Mic 3 and Mic 1, between what is “of the performance” and what is incidental sound; the latter becomes part of each audiotext, according to the specific and variable situation of that audiotext in space and media. They are, in Bernstein’s description of the sounded poem, “always at the edge of semantic excess,” (13) open and inviting to what lies beyond. In recognition of the impossibility of putting the extra-semantic into language, I’ve selected descriptors that are relative and evidently subjective. Relative descriptors indicate the extent (“low,” “mid,” “high,” “soft,” “hard”) of qualities including pitch, volume, and onset/offset. I also variously use other relative descriptors (e.g. “sporadic”). This entails a loss of rigor—someone seeking to, for instance, digitally reconstruct the sounds solely from my annotations would find them wildly insufficient—but it opens up my annotations to multiple sonic possibilities, following the model of Oana’s performance.
	Relative language is also reflective of the process I used to determine elements like pitch and volume. Using Audacity, I created waveforms and spectrograms for each audiotext. This is a multi-view of the waveform (top) and spectrogram (bottom) for each of the channels in the Mic 3 recording:
</p>

<img width="908" alt="Screenshot of Audacity interface, including waveform and spectogram correcponding to Mic 3." src="https://github.com/sarah-freeman/operator-annotations/assets/142846974/3fb9dfbc-b8bf-4a36-b0d6-0ce90f269a9d">

