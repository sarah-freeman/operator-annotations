---
layout: default
title: Introduction
permalink: introduction
---
<!-- Add an essay or interpretive material below this line,
using HTML or markdown.  Do not modify this file above this line -->
## Introduction

<p>Annotation is one of John Unsworth’s scholarly primitives: processes essential for research, pedagogy, and interpretation. Although primarily considered in relation to printed texts, it’s also useful for scholars working with digital sound recordings, wherein various forms of annotation can serve various functions, from metadata that provides contextualizing information, to transcriptions which remediate sound, to personal, timestamped notes. These forms of annotation are specific to different contexts, such as the format of the audio recording, the affordances of the repository in which it is displayed, and the listening paradigm of the creator. Because annotation is not an ahistorical, incidental consequence of interacting with audio online but rather a situated artifact of a given listening experience, it holds within it also the possibility of critical reflexivity about an audiotext and its various forms of mediation. Typical theories of annotation tend not to consider the affordances of annotation when it is not an artifact of “authorial intention,” (as in paratext and occasionally marginalia) or else a means of making an item “system-aware” (as in metadata schemes used in online repositories) (Clement and Fischer, par. 1, par. 3). However, recent scholarship has produced both theories (e.g. Clement and Fischer’s “audiated annotation”) and case studies (e.g. The SpokenWeb Digital Anthology, published on AVAnnotate) illustrating the critical significance of annotation as a practice in itself. Based on these developments, I use annotation in this project as a means to think reflexively about how I represent the specific features of a given audiotext and the specific forms of mediation that inform my listening. I find that a consciously reflexive annotation process leads to annotations that reflect their own situatedness.</p>

<p>	This AVAnnotate project is a case study in reflexive annotation. Here, I present annotations of three recordings of Oana Avasilichioaei’s performance at the 2019 SpokenWeb Symposium. My annotation is grounded in audiotextual features, including polyphony, semiotic features, and sonic stratification; and markers of my own mediated listening practice, which was focused on relative differences through use of the digital editing application Audacity. Although each recording is of the same performance, they come from recording devices placed differently in the room. By reflexively attending to the audiotextual and mediated specificities of each audiotext, I produce annotations that highlight the distinct features of each, rather than reducing them to a common or essential set of sounds. This case study is an indication that reflexive annotation can assert audiotextual multiplicity, and highlight the multimedial and (frequently) unauthorized elements that make it possible to listen to and annotate audio. I begin by discussing how features of the audiotexts, such as polyvocality, ambient sound, vocal modulation, fluctuations, semantic features, and novel sounds, inform my selection of a common annotation genre and language, but varying topics and layers. Next, I address how the technological specificities of my listening experience inform my use of relative language and changing precision in my timestamps. Each topic I discuss includes examples from my “Operator” annotations. </p>

## Audiotextual Features
<p>While there are several potential forms audio annotation can take, the selection of a given form and vocabulary to describe sound isn't typically discussed as a result of interpretive and/or critical strategies. Rather, it’s a result of functional needs (e.g. transcripts for a podcast) or technological affordances (e.g. the parameters of a given metadata scheme). However, tools like AVAnnotate, which support a variety of forms of annotation, allow users creative latitude to define their own format (within certain limits) and vocabulary. Different ways of annotating sound are informed by different critical histories and interpretive goals. For instance, describing sound through the lens of prosody situates annotations in a specific form of poetic criticism which assumes consistency and exactitude (Bernstein 14). Another form, transcription of semantic language, in “demand[ing] a certain degree of envoicement from the reader,” may obscure the semiotic qualities of the audiotext (Camlot and Mitchell). Furthermore, elements like timestamps may represent different temporalities depending on whether they’re strictly chronological or allow for gaps and overlaps in time. The critical significance of forms and vocabularies of annotation, then, is not only in their affordances as means of representation, but also in how they constitute sound. In the following discussion, I describe some elements of the “Operator” audiotexts and explain how I used them to inform the structure of my annotations.</p>

<p><i>
Layered timestamps and indexes
</i></p>
<p>At any one moment in any of the audiotexts, there are multiple layers of sound. These range from an electronic bass tone to voices to static distortion. They become audible and inaudible at different times, change at varying intervals—in these ways, they exist in time independently from each other. For this reason, I’ve timestamped them independently. This creates a non-chronological temporality in my timestamps, in that a sound cannot be said to have completely occurred before the next sound, only to have begun before the next. Consider, for instance, this section from Mic 1:</p>
<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:01:40.63</td>
    <td>0:02:92.26</td> 
    <td>Sporadic mid-volume rolling sound, like a pool ball rolling across a table</td>
  </tr>
  <tr>
    <td>0:01:54.0</td>
    <td>0:01:54.4</td> 
    <td>Quiet breathy/blowing sound</td>
  </tr>
  <tr>
    <td>0:01:59.193</td>
    <td>0:01:59.59</td> 
    <td>Loud tap, like two pool balls colliding</td>
  </tr>
</table>
<p>One moment in time, such as 0:01:50, corresponds to multiple indices and thus has multiple layers, none of which—in the Mic 1 audiotext—are subordinate to each other even when they are infrequent/sporadic (as in the first entry above), quiet (as in the second) or short (as in the third). Mic 3 has similar interactions between layers of sounds, although they blend together a bit more, leading to more annotations that group sounds together.
Mic 2 is also polyphonic, but it has clearer audio quality and consequently clearly defined layers of sound. Because the layers of sound are more distinct, I am able to identify layers not just based on time (i.e. through simultaneous timestamps) but also by shared sonic characteristics between sounds at different points in the audiotext. These shared qualities inform my use of indexes to denote four general categories of sound: speaking voices, silence, electronic tones and distortions, and friction sounds. The latter two are represented in this sample:
</p>
<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:01:35.78</td>
    <td>0:02:03.32</td> 
    <td>Gravelley, crunching sound beginning with a period of ~0.5 second. Low to mid pitch and volume. Period becomes longer and volume decreases through this interval.</td>
    <td>Friction sounds</td>
  </tr>
  <tr>
    <td>0:01:40.66</td>
    <td>0:02:02.04</td> 
    <td>Clear scratch sound with varying pitch and generally low volume, like a wooden stick tracing across a wooden table.</td>
    <td>Friction sounds</td>
  </tr>
  <tr>
    <td>0:02:10.01</td>
    <td>0:02:11:71</td> 
    <td>Polyphonic electronic tone crescendos to high volume</td>
    <td>Electronic tones and distortions</td>
  </tr>
</table>
<p>By attending to the polyphonic nature of each of these audiotexts, and the distinct manner in which sounds stratify, blend, and remediate each other in these audiotexts, I reflect the variances in sonic layering in each.</p>

<p><i>Focus on semiotic features</i></p>
<p>	In each of the audiotexts, the majority of the sounds are not symbolic speech. And, when spoken language is audible, it’s complicated by semiotic features, whether from other layers of sound or striking voice modulation. For this reason, my annotation focuses on semiotic features of sound. This is most evident in my descriptions of speech, which use terms of volume, pitch, onset/offset, tone, and (occasionally) echo. Here I draw on Charles Bernstein’s description of a “poetic mode of listening,” in which there is “an oscillation (or temporal overlap) between the materially present sound…and the absent meaning” (18). It’s for this reason, also, that I rarely use spacial descriptors (i.e. background, foreground) and I don’t often attribute sounds to specific sources. This practice is informed, in part, by Michel Chion’s reduced listening, which centers around “inherent qualities of sounds,” (29) and which Chion places at odds with causal listening (i.e. listening for information about the source) and semantic listening (i.e. listening for linguistic meaning). 
	While Chion distinguishes between these as three separate forms of listening, my annotations are closer to, in Bernstein’s description, “oscillati[ng]” between them. This is true in the sense that none of my descriptors are directly indicative of innate features of sound, but instead the necessity of essentializing certain features of it, using constructed ways of describing sound (e.g. volume and pitch), in order to be able to write about it. It’s also true in the sense that there are causal and semantic descriptions mixed into my semiotic descriptions. In annotations for each audiotext, I use timbre descriptors like “electronic,” “brassy,” “ and “crunching,” which imply causes even if they don’t state them outright. When I use more specific images, I make them similes, as in “mid-pitch clicking sound, like ball bearings colliding” (Mic 2). 
Because Mic 1 has the clearest speech, it has more descriptors of symbolic features of speech, as in the following example:
</p>
<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:03:12.83</td>
    <td>0:03:43.5</td> 
    <td>One mid pitch voice speaking. Little intonation (i.e. most syllables are stressed), resulting in choppy-sounding words (e.g. ve-hi-cules). Shout pauses between words. Call-and-response form (e.g. "...right? Yeah, that's what it looks like to me.") suggests two speakers, but there's only one voice. Terms that presumably refer to descriptive rather than spoken text (e.g. "unintelligible," "explicative") included within flow of speech, uttered by same speaker.</td>
  </tr>
</table>
<p>Here, I discuss a symbolic “call and response form” which is contradicted by semiotic features denoting a single voice. Additionally, I isolate words which I assume to “refer to descriptive rather than spoken text,” implicitly referring to their disruption of the syntactic structures of the speech, and to the forms of audio transcription which typically use descriptors like [unintelligible]. In this case, including these descriptions highlights how the audiotext draws attention to the blurry line between the semiotic and the semantic: does “unintelligible” function the same way as other symbolic words in this context, or is it closer to an index? When questions like this arise from the audiotext, they are obscured by a sole focus on semiotic features of speech. I alter my descriptions, when needed, to fit these complexities of the audiotext. 
Bernstein’s discussion of “absent meaning” is particularly relevant to the Mic 3 audiotext, in which spoken words are frequently inaudible (in the sense that they can’t be made out as symbolic words). To seek the symbolic in these sounds really does feel like reaching for something that’s not present among a plethora of “materially present sound[s],” more than are in either of the other audiotexts. For this reason, I’ve foregone the descriptor “Oana’s voice” in my annotations of this audiotext, not because her voice is unrecognizable, but because this descriptor is causative (i.e. sound is attributed to voice, which is ultimately attributed to a causative speaker) and privileges the voice relative to other sounds, when it does not function in a privileged manner in this audiotext itself, but rather is mixed into the sonic din. Because Oana’s voice is separated by volume, pitch, and clarity from the other sounds in the Mic 1 and Mic 2 audiotexts, I use the “Oana’s voice” descriptor to indicate this. However, I maintain my focus on semiotic features of sound across all of my annotations in recognition of the skillful manner in which she manipulates her voice, imbuing it with semiotic meaning.
Of course, the semiotic elements of a voiced performance are not predicated on authorial intent. What I’m suggesting here is, instead, that this audiotext is in many ways about the relationship between the symbolic and the semiotic; at what point does whispered offset become simply a whisper in itself, or when (if ever) does a repeated word lose its meaning? The Operator audiotexts interrogate the constructed boundaries between the semiotic and the semantic in complex, intermingling ways, which I can only begin to approach in my written annotations.</p>

<p><i>General and relative language</i></p>
<p>The difficulty of describing literary sound has been noted by Bernstein, who writes that “our technical vocabulary strains at accounting for more than a small portion of the acoustic activity of the sounded poem” (15). The Operator audiotexts, on the other hand, are open to unauthorized sound—because of the polyvocal, variable performance style, there is no clear delimitation, particularly in Mic 3 and Mic 1, between what is “of the performance” and what is incidental sound; the latter becomes part of each audiotext, according to the specific and variable situation of that audiotext in space and media. They are, in Bernstein’s description of the sounded poem, “always at the edge of semantic excess,” (13) open and inviting to what lies beyond. In recognition of the impossibility of putting the extra-semantic into language, I’ve selected descriptors that are relative and evidently subjective. Relative descriptors indicate the extent (“low,” “mid,” “high,” “soft,” “hard”) of qualities including pitch, volume, and onset/offset. I also variously use other relative descriptors (e.g. “sporadic”). This entails a loss of rigor—someone seeking to, for instance, digitally reconstruct the sounds solely from my annotations would find them wildly insufficient—but it opens up my annotations to multiple sonic possibilities, following the model of Oana’s performance.
	Relative language is also reflective of the process I used to determine elements like pitch and volume. Using Audacity, I created waveforms and spectrograms for each audiotext. This is a multi-view of the waveform (top) and spectrogram (bottom) for each of the channels in the Mic 3 recording:
</p>

<img width="908" alt="Screenshot of Audacity interface, including waveform and spectogram correcponding to Mic 3." src="https://github.com/sarah-freeman/operator-annotations/assets/142846974/3fb9dfbc-b8bf-4a36-b0d6-0ce90f269a9d">

<p>I base my descriptions on my perception of the audio and, secondarily, what was represented in the images. If I describe a sound as high volume, for instance, it’s because it sounded loud relative to the surrounding sounds, but also because there was likely a peak on the waveform at that point. When I struggled to discern features of the sound, I relied more heavily on the waveform and spectrogram. I don’t use numerical units like decibels—although they are measured by Audacity—because those units aren’t reflective of the way in which I used the technology, which wasn’t to isolate “absolute” features of the sound at a given moment in time, but to compare the sound across time.</p>
<p>Furthermore, using precise units presumes precision that I didn’t have in my listening experience; for instance, I used Bluetooth headphones, which likely compressed the sound as I heard it, so what I saw on Audacity—even by the numbers—is not necessarily representative of what I heard.
One benefit of these descriptors is that I can use the same terms to describe a variety of sounds; unlike technical vocabularies, they are general enough to be flexible. Another is that, because they’re relative, making sense of any given annotation requires engagement with the surrounding annotations and perhaps also the audiotext itself. For instance, a high volume sound in the Mic 2 recording is louder than the Mic 1 recording, because the Mic 2 recording is generally louder. This both centers polyphony and de-centers meaning from any single location, written or sounded. This, I would argue, is a descriptive practice which, while lacking in rigor and not suited for all purposes, is consistent with the polyphony, fluidity, and openness of the audiotexts themselves.</p>

## Mediation

<p> In the above discussion, I’ve moved from discussing characteristics of the audiotexts to the characteristics of my technologically mediated listening practice. My listening and my annotations are informed by my use of Audacity, an audio editing software, to navigate the recordings. Audio technologies, and Jonathan Sterne notes, are not value-neutral; rather, the quantification of sound and the representation of the listening subject “through ensembles of hearing equipment” led to the construction of “noise” as social and cultural issue (70). It’s this type of technology that makes it easy to see, with a glance at a spectrogram, that Mic 3 is far noisier than Mic 2, and to forget that this is just one ideologically and socially-situated way of viewing a visual representation of sound.
In this section, I discuss how my timestamps of highly-variable length, and my reference to a common time axis for all the recordings are specific to the technological mediations of my listening experience. </p>

<p><i>Excerpting</i></p>

<p>My annotations span an excerpt of 00:06:35 from the full performance, which included several performers and lasted roughly an hour and a half. Oana’s original performance was about 16 minutes; the excerpted portion represents the end of her performance (it cuts off right before the closing applause). I chose this portion because it is generally representative of the varieties of voices and sounds present throughout the performance. It is long enough to give a sense of the performance, and short enough to allow me to provide detailed annotations.</p>

<p>My use of this excerpt, and the ways in which I approach it, are significant to the situation of each of the audiotexts in relation to each other and in relation to the annotations. My excerpted section occurs at different start and end points in each of the full-length audiotexts, owing to varying start times. Rather than reflect these times in my timestamps, I timestamp based on the excerpted duration, from 0:00:00 to 00:06:35, for all three audiotexts. This choice is significant because it reflects the remediation of the audio object in this context and facilitates comparison between the audiotexts.</p>

<p>To begin timestamping at 0:00:00 is to acknowledge a temporality (and set of temporal contingencies) that is distinct to the excerpted audiotexts and the context in which I place them. In the first instance, this is distinct from the temporality of the live performance, which Friedrich Kittler calls “real time,” and cannot be represented as a duration. To represent a time period as a duration entails a set of technologically mediated, time-based practices like pausing, clipping, and rewinding, which Kittler refers to as “time axis manipulation.” Both the original audio clips and my excerpted versions engender not only the possibility of, but the active use of time axis manipulation. The presence of an original and an excerpt implies the use of some technological means of cutting and then saving an excerpted form, which by this process of time axis manipulation is not merely a derivative of the original but a new audio object in its own right. Thus the excerpt is both predicated on and subject to time axis manipulation in this project. </p>

<p>Having a standard duration across all three audiotexts also facilitates comparison between them at a specific point in time. Somewhat paradoxically, ease of comparison is advantageous in highlighting the singularity of each of these audiotexts, because it makes clear the abundant differences between the annotations at a given time across all audiotexts. It’s also true that having a standard time axis across all three audiotexts obscures the different durations between the original recordings, and seems to gesture to the “real time” of the live performance (without preserving it). Because the excerpted audiotexts implicate the common situation of the live performance and facilitate comparison of their differences, this provokes consideration of the various audiotextual and mediation elements that may lead to their differences.</p>

<p><i>Variable-length timestamps</i></p>

<p>To the extent that my annotations are also a visual (i.e. written text) interpretation of sound, I’ve used Audacity to preserve what may now be called “visual noise.” All of these audiotexts are noisy, in the sense that they perpetuate uncontrolled, unauthorized, and (in the case of Mic 1 and Mic 3) spatially resonant sound. Sometimes, I group these sounds together in a single annotation, as in the following from Mic 3:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:02:50.64</td>
    <td>0:032:59.2</td> 
    <td>Mid pitch, mid volume fluctuating static tone, with various low volume mid pitch cracking sounds at inconsistent intervals</td>
  </tr>
</table>

<p>Other times, I timestamp them individually, as in this annotation from Mic 1:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:01:01.598</td>
    <td>0:01:01.8</td> 
    <td>Pop with hard onset and soft offset</td>
  </tr>
</table>

<p>A timestamp under one second is a bit ridiculous in terms of what’s useful or typical, but it’s helpful in making evident my use of Audacity, and in its atypicality also indicating the subjectivity of using audio technology to gain precise access to “noisy” signals, like underlying static (as in the first example) and acute pops (as in the second), often to get rid of them. 
</p>

<p>Somewhat unexpectedly, there are more “visual noise” timestamps in my Mic 1 annotations than either of the other annotations, although in visual representations, it appears to have less noise than Mic 3. This is because the noise was more consistent in terms of both presence and sonic characteristics in the Mic 3 audio, and therefore I could use longer timestamps for these annotations. In Mic 1, however, it was mostly short pops and cracks which weren’t consistent with the surrounding sounds, and for that reason I annotated them separately. Mic 2 has a mix of both.</p>

<p><i>Variable significant figures in timestamps</i></p>
<p>I’ve indicated my relative level of certainty about the timestamps by varying my significant figures in each timestamp. These are both a gesture to the complexities of the sounds in these audiotexts, which may cut off in an instance or fade away over time, with little indication of the discrete endpoint implied by an exact timestamp. By varying the relative precision of my timestamps, I implicate both the technological affordances of Audacity (which facilitates precision up to three decimal points) and the role of my own judgment in determining when a sound can be said to start and end.</p>

## Conclusion

<p>In this introduction, I explored how the central axioms of an audio annotation process can be determined not by usage conventions, but instead reflexive to the audiotext and its mediation. This led to distinctions in how I used layering, semiotic descriptors, and relative language to reproduce varying levels of polyphony, semiotic features, and openness to unauthorized sound, to annotate each of the “Operator” audiotexts. It also led to annotations that are evidently situated in my own listening practice, a somewhat imprecise use of a digital audio technology that purports to be very precise. This I model through varying precision in my timestamps, and non-technical language.</p>

<p>Ultimately, these annotations aren't an end in and of themselves. I mean this in two senses: first, with reference to the audiotextual and mediation features—and lenses for looking at these features—that I didn't include in this project. This is sometimes due to the limits of my own knowldge, for instance, in my limited discussion of audio compression. It's also due, in part, to the audiotexts; someone working with a monovocal performance would necessarily have different considerations, which would likely lead to different critical parameters in their project. But, it's also due to this being a largely exploratory project. More developed works would likely more clearly articulate a specific critical lens, which I didn't do here.
	In the second sense, these annotations don't exhaust the possibilities of annotating "Operator" audiotexts themselves. Because my annotations are specific to my own listening experience (and critical limitations), they shouldn't serve as a stabilizing document but instead one of interpretation, which may be countered, built upon, repurposed or used critically in another way. </p>

<p><i>Works Cited</i>
“Audiated Annotation from the Middle Ages to the Open Web.” DHQ: Digital Humanities Quarterly, 2021, www.digitalhumanities.org/dhq/vol/15/1/000512/000512.html. 
	
Bernstein, Charles. “Introduction.” Close Listening: Poetry and the Performed Word, Oxford University Press, New York, NY, 1998, pp. 3–26. 

Camlot, Jason, and Christine Mitchell. “Amodern 4: The Poetry Series.” Amodern, vol. 4, Mar. 2015, https://doi.org/https://amodern.net/issues/amodern-4/. 

Chion, Michel. “The Three Listening Modes.” Audio-Vision: Sound on Screen, translated by Claudia Gorbman, Second ed., Columbia University Press, New York, NY, 2019, pp. 22–34. 

Sterne, Jonathan. “Hearing.” Keywords in Sound, Duke University Press, 2015, pp. 65–77. 

Unsworth, John. “Scholarly Primitives: What Methods Do Humanities Researchers Have in Common, and How Might Our Tools Reflect This?” John M. Unsworth: Conference Papers and Presentations, University of Virginia, 13 May 2000, people.brandeis.edu/~unsworth/Kings.5-00/primitives.html. </p>
